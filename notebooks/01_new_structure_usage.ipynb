{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hypopredict Package: New Structure Usage Guide\n",
    "\n",
    "This notebook demonstrates the complete usage of the refactored hypopredict package.\n",
    "\n",
    "**Contents:**\n",
    "1. Setup and Imports\n",
    "2. Data Loading Examples\n",
    "3. Person Class Usage\n",
    "4. Data Chunking\n",
    "5. Feature Extraction\n",
    "6. Cross-Validation Setup\n",
    "7. Complete Training Pipeline\n",
    "8. PyTorch Dataset Creation\n",
    "9. Error Handling and Debugging\n",
    "10. GCS/Cloud Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, let's import all necessary modules and configure our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# hypopredict package imports\n",
    "import hypopredict.compressor as comp\n",
    "import hypopredict.train_test_split as tts\n",
    "import hypopredict.feature_extraction as fe\n",
    "from hypopredict.person import Person\n",
    "from hypopredict.cv import CV_splitter\n",
    "from hypopredict import params\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "print(\"✓ All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Check environment configuration\n",
    "glucose_path = os.getenv('GLUCOSE_PATH')\n",
    "ecg_path = os.getenv('ECG_PATH')\n",
    "\n",
    "print(\"Environment Configuration:\")\n",
    "print(f\"GLUCOSE_PATH: {glucose_path}\")\n",
    "print(f\"ECG_PATH: {ecg_path}\")\n",
    "print()\n",
    "\n",
    "if glucose_path and Path(glucose_path).exists():\n",
    "    print(f\"✓ Glucose path exists\")\n",
    "else:\n",
    "    print(\"⚠ Warning: Glucose path not configured or doesn't exist\")\n",
    "    \n",
    "if ecg_path and Path(ecg_path).exists():\n",
    "    print(f\"✓ ECG path exists\")\n",
    "else:\n",
    "    print(\"⚠ Warning: ECG path not configured or doesn't exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading Examples\n",
    "\n",
    "The hypopredict package supports loading data from multiple sources:\n",
    "- Google Drive (for quick sharing)\n",
    "- Local filesystem (for production)\n",
    "- GCS buckets (for cloud training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load Glucose Data from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load glucose data for person 1 from Google Drive\n",
    "# This is useful for quick prototyping and sharing\n",
    "\n",
    "print(\"Loading glucose data from Google Drive...\")\n",
    "try:\n",
    "    glucose_df = comp.gdrive_to_pandas(comp.GLUCOSE_ID_LINKS[0])\n",
    "    print(f\"✓ Loaded glucose data: {glucose_df.shape}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    display(glucose_df.head())\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Could not load from Google Drive: {e}\")\n",
    "    print(\"This may require internet access or Google Drive permissions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Identify Hypoglycemic (HG) Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify HG events: glucose < 3.9 mmol/L for at least 15 minutes\n",
    "\n",
    "if 'glucose_df' in locals():\n",
    "    print(\"Identifying HG events...\")\n",
    "    hg_events = comp.identify_hg_events(\n",
    "        glucose_df,\n",
    "        threshold=3.9,      # mmol/L\n",
    "        min_duration=15,    # minutes\n",
    "        cgm_only=True       # Use only CGM readings\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ HG events identified: {hg_events.shape}\")\n",
    "    print(f\"\\nHG event statistics:\")\n",
    "    print(f\"  Total time points: {len(hg_events)}\")\n",
    "    print(f\"  HG time points: {hg_events['is_hg'].sum()}\")\n",
    "    print(f\"  HG proportion: {hg_events['is_hg'].mean():.2%}\")\n",
    "    print(f\"  Number of HG onsets: {hg_events['onset'].sum()}\")\n",
    "    \n",
    "    display(hg_events[hg_events['onset'] == 1].head())\n",
    "else:\n",
    "    print(\"⚠ Glucose data not loaded. Skipping HG event identification.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Person Class Usage\n",
    "\n",
    "The `Person` class provides a clean interface for managing patient data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Initialize and Load Patient Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Person object\n",
    "person_id = 1\n",
    "\n",
    "if ecg_path:\n",
    "    person = Person(ID=person_id, ecg_dir=ecg_path)\n",
    "    print(f\"✓ Person object created: ID={person.ID}\")\n",
    "    print(f\"  ECG directory: {person.ecg_dir}\")\n",
    "else:\n",
    "    print(\"⚠ ECG path not configured. Cannot create Person object.\")\n",
    "    person = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Load Glucose Data and Identify HG Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load glucose data using Person class\n",
    "# This method handles data loading and HG event identification\n",
    "\n",
    "if person and glucose_path:\n",
    "    try:\n",
    "        print(\"Loading HG data...\")\n",
    "        person.load_HG_data(\n",
    "            glucose_src='local',    # or 'gdrive'\n",
    "            min_duration=15,        # minutes\n",
    "            threshold=3.9           # mmol/L\n",
    "        )\n",
    "        print(f\"✓ HG data loaded successfully\")\n",
    "        print(f\"  HG events shape: {person.hg_events.shape}\")\n",
    "        print(f\"  HG proportion: {person.hg_events['is_hg'].mean():.2%}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"⚠ Glucose data file not found: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Error loading glucose data: {e}\")\n",
    "elif person:\n",
    "    print(\"⚠ Glucose path not configured. Using Google Drive instead.\")\n",
    "    try:\n",
    "        person.load_HG_data(glucose_src='gdrive')\n",
    "        print(f\"✓ HG data loaded from Google Drive\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Error loading from Google Drive: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Load ECG Data for a Specific Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ECG data for day 4\n",
    "day = 4\n",
    "\n",
    "if person and ecg_path:\n",
    "    try:\n",
    "        print(f\"Loading ECG data for day {day}...\")\n",
    "        person.load_ECG_day(day=day, warning=True)\n",
    "        \n",
    "        print(f\"✓ ECG data loaded successfully\")\n",
    "        print(f\"  Shape: {person.ecg[day].shape}\")\n",
    "        print(f\"  Columns: {list(person.ecg[day].columns)}\")\n",
    "        print(f\"  Time range: {person.ecg[day].index[0]} to {person.ecg[day].index[-1]}\")\n",
    "        print(f\"  Duration: {person.ecg[day].index[-1] - person.ecg[day].index[0]}\")\n",
    "        \n",
    "        display(person.ecg[day].head())\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"⚠ ECG data file not found: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Error loading ECG data: {e}\")\n",
    "else:\n",
    "    print(\"⚠ Person object not initialized or ECG path not configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Chunking\n",
    "\n",
    "Split continuous time-series data into overlapping chunks for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Define Chunk Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define chunking parameters\n",
    "chunk_size = pd.Timedelta(minutes=5)  # 5-minute chunks\n",
    "step_size = pd.Timedelta(minutes=1)   # 1-minute step (4 minutes overlap)\n",
    "\n",
    "print(f\"Chunk configuration:\")\n",
    "print(f\"  Chunk size: {chunk_size}\")\n",
    "print(f\"  Step size: {step_size}\")\n",
    "print(f\"  Overlap: {chunk_size - step_size} ({(1 - step_size/chunk_size)*100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Chunk a Single DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk the ECG data we loaded earlier\n",
    "\n",
    "if person and day in person.ecg:\n",
    "    print(f\"Chunking ECG data for day {day}...\")\n",
    "    chunks = tts.chunkify_df(\n",
    "        df=person.ecg[day],\n",
    "        chunk_size=chunk_size,\n",
    "        step_size=step_size\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Data chunked successfully\")\n",
    "    print(f\"  Number of chunks: {len(chunks)}\")\n",
    "    print(f\"  First chunk shape: {chunks[0].shape}\")\n",
    "    print(f\"  First chunk time range: {chunks[0].index[0]} to {chunks[0].index[-1]}\")\n",
    "    print(f\"  Last chunk shape: {chunks[-1].shape}\")\n",
    "else:\n",
    "    print(\"⚠ ECG data not loaded. Skipping chunking.\")\n",
    "    chunks = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Chunk a Single Person-Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk using person_day notation (person_id * 10 + day)\n",
    "person_day = 14  # Person 1, Day 4\n",
    "\n",
    "if ecg_path:\n",
    "    try:\n",
    "        print(f\"Chunking person-day {person_day}...\")\n",
    "        pd_result, chunks_pd = tts.chunkify_day(\n",
    "            person_day=person_day,\n",
    "            chunk_size=chunk_size,\n",
    "            step_size=step_size,\n",
    "            ecg_dir=ecg_path\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ Person-day {pd_result} chunked successfully\")\n",
    "        print(f\"  Number of chunks: {len(chunks_pd)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Error chunking person-day: {e}\")\n",
    "        chunks_pd = []\n",
    "else:\n",
    "    print(\"⚠ ECG path not configured. Skipping person-day chunking.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Chunk Multiple Person-Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk multiple person-days at once\n",
    "# Note: This requires actual data files to exist\n",
    "\n",
    "if ecg_path:\n",
    "    # Use a small subset for demonstration\n",
    "    demo_days = params.DEMO_DAYS  # Days with higher HG proportion\n",
    "    print(f\"Chunking {len(demo_days)} demo days: {demo_days}\")\n",
    "    \n",
    "    try:\n",
    "        chunks_all = tts.chunkify(\n",
    "            person_days=demo_days,\n",
    "            chunk_size=chunk_size,\n",
    "            step_size=step_size,\n",
    "            ecg_dir=ecg_path\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ All days chunked successfully\")\n",
    "        for pd, chunks in chunks_all.items():\n",
    "            print(f\"  Day {pd}: {len(chunks)} chunks\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Error chunking multiple days: {e}\")\n",
    "        print(\"This is expected if data files don't exist locally.\")\n",
    "        chunks_all = {}\n",
    "else:\n",
    "    print(\"⚠ ECG path not configured. Skipping multiple-day chunking.\")\n",
    "    chunks_all = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Extraction\n",
    "\n",
    "Extract features from chunks for machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Extract Statistical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract statistical features (mean, std, min, max, quantiles, skew, kurtosis)\n",
    "\n",
    "if chunks:\n",
    "    print(\"Extracting statistical features...\")\n",
    "    stat_features = fe.extract_features(chunks[:10])  # Use first 10 chunks for demo\n",
    "    \n",
    "    print(f\"✓ Statistical features extracted\")\n",
    "    print(f\"  Feature matrix shape: {stat_features.shape}\")\n",
    "    print(f\"  Number of features: {stat_features.shape[1]}\")\n",
    "    print(f\"\\nFeature columns (first 10):\")\n",
    "    print(list(stat_features.columns[:10]))\n",
    "    \n",
    "    display(stat_features.head())\n",
    "else:\n",
    "    print(\"⚠ No chunks available. Skipping feature extraction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Extract ECG Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract ECG-specific features (R-peaks, heart rate, RR intervals)\n",
    "\n",
    "if chunks:\n",
    "    print(\"Extracting ECG features...\")\n",
    "    ecg_features = fe.extract_ecg_features(\n",
    "        chunks[:5],  # Use first 5 chunks for demo (ECG processing is slower)\n",
    "        ecg_column='EcgWaveform',\n",
    "        sampling_rate=250,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✓ ECG features extracted\")\n",
    "    print(f\"  Feature matrix shape: {ecg_features.shape}\")\n",
    "    print(f\"  Feature columns: {list(ecg_features.columns)}\")\n",
    "    \n",
    "    display(ecg_features)\n",
    "else:\n",
    "    print(\"⚠ No chunks available. Skipping ECG feature extraction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Extract HRV Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Heart Rate Variability (HRV) features\n",
    "# Note: HRV extraction may produce warnings for chunks with insufficient R-peaks\n",
    "\n",
    "if chunks:\n",
    "    print(\"Extracting HRV features...\")\n",
    "    try:\n",
    "        hrv_features_list = fe.extract_hrv_features(\n",
    "            chunks[:3],  # Use first 3 chunks for demo (HRV processing is slow)\n",
    "            ecg_column='EcgWaveform',\n",
    "            sampling_rate=250\n",
    "        )\n",
    "        \n",
    "        # Combine HRV features into a single DataFrame\n",
    "        hrv_features = pd.concat(hrv_features_list, ignore_index=True)\n",
    "        \n",
    "        print(f\"✓ HRV features extracted\")\n",
    "        print(f\"  Feature matrix shape: {hrv_features.shape}\")\n",
    "        print(f\"  Number of HRV features: {hrv_features.shape[1]}\")\n",
    "        \n",
    "        display(hrv_features.head())\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Error extracting HRV features: {e}\")\n",
    "else:\n",
    "    print(\"⚠ No chunks available. Skipping HRV feature extraction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Extract Combined Features (Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all features at once (more efficient)\n",
    "\n",
    "if chunks:\n",
    "    print(\"Extracting combined features...\")\n",
    "    combined_features = fe.extract_combined_features_sequential(\n",
    "        chunks[:5],  # Use first 5 chunks for demo\n",
    "        ecg_column='EcgWaveform',\n",
    "        sampling_rate=250,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✓ Combined features extracted\")\n",
    "    print(f\"  Feature matrix shape: {combined_features.shape}\")\n",
    "    print(f\"  Number of features: {combined_features.shape[1]}\")\n",
    "    print(f\"  NaN values: {combined_features.isna().sum().sum()}\")\n",
    "    \n",
    "    # Handle NaN values\n",
    "    combined_features_clean = combined_features.fillna(0)\n",
    "    print(f\"  After filling NaN with 0: {combined_features_clean.isna().sum().sum()} NaN values\")\n",
    "    \n",
    "    display(combined_features_clean.head())\n",
    "else:\n",
    "    print(\"⚠ No chunks available. Skipping combined feature extraction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configuration Management (params.py)\n",
    "\n",
    "The `params` module provides centralized dataset configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore dataset parameters\n",
    "\n",
    "print(\"Dataset Configuration:\")\n",
    "print(f\"\\nTotal days available: {len(params.ALL_DAYS)}\")\n",
    "print(f\"Training days: {len(params.TRAIN_DAYS)}\")\n",
    "print(f\"Test days: {len(params.TEST_DAYS)}\")\n",
    "print(f\"Demo days: {len(params.DEMO_DAYS)}\")\n",
    "print(f\"Invalid days: {len(params.INVALID_DAYS)}\")\n",
    "print(f\"\\nDays with HG events: {len(params.HG_DAYS)}\")\n",
    "print(f\"Days without HG events: {len(params.ZERO_DAYS)}\")\n",
    "\n",
    "print(f\"\\nTraining days: {params.TRAIN_DAYS}\")\n",
    "print(f\"Test days: {params.TEST_DAYS}\")\n",
    "print(f\"Demo days: {params.DEMO_DAYS}\")\n",
    "print(f\"Invalid days (skip these): {params.INVALID_DAYS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cross-Validation Setup\n",
    "\n",
    "Use the `CV_splitter` class for proper train-validation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize cross-validation splitter\n",
    "\n",
    "if ecg_path and glucose_path:\n",
    "    cv_splitter = CV_splitter(\n",
    "        ecg_dir=ecg_path,\n",
    "        glucose_src='local',\n",
    "        n_splits=5,\n",
    "        random_state=17\n",
    "    )\n",
    "    \n",
    "    print(\"✓ CV splitter initialized\")\n",
    "    print(f\"  Number of splits: {cv_splitter.n_splits}\")\n",
    "    print(f\"  Random state: {cv_splitter.random_state}\")\n",
    "else:\n",
    "    print(\"⚠ Paths not configured. Cannot initialize CV splitter.\")\n",
    "    cv_splitter = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate cross-validation splits\n",
    "\n",
    "if cv_splitter:\n",
    "    print(\"Generating CV splits...\")\n",
    "    splits = cv_splitter.get_splits(params.TRAIN_DAYS)\n",
    "    \n",
    "    print(f\"✓ Splits generated\")\n",
    "    print(f\"  Number of splits: {len(splits)}\")\n",
    "    for i, split in enumerate(splits):\n",
    "        print(f\"  Fold {i+1}: {len(split)} days - {split}\")\n",
    "else:\n",
    "    print(\"⚠ CV splitter not initialized. Skipping split generation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate splits (check for HG events)\n",
    "# Note: This requires loading glucose data for each day\n",
    "\n",
    "if cv_splitter and 'splits' in locals():\n",
    "    print(\"Validating splits...\")\n",
    "    try:\n",
    "        valid, hg_proportions = cv_splitter.validate(splits, verbose=True, warning=False)\n",
    "        \n",
    "        print(f\"\\n✓ Validation complete\")\n",
    "        print(f\"\\nSplit HG proportions:\")\n",
    "        for i, prop in enumerate(hg_proportions):\n",
    "            print(f\"  Fold {i+1}: {prop:.2%} HG\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Error validating splits: {e}\")\n",
    "        print(\"This is expected if glucose data files don't exist locally.\")\n",
    "else:\n",
    "    print(\"⚠ Splits not generated. Skipping validation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Complete Training Pipeline Example\n",
    "\n",
    "Putting it all together: chunk data, extract features, and prepare for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete pipeline function\n",
    "\n",
    "def prepare_training_data(person_days, ecg_dir, chunk_size, step_size, verbose=True):\n",
    "    \"\"\"\n",
    "    Complete pipeline: chunk data and extract features.\n",
    "    \n",
    "    Args:\n",
    "        person_days: List of person-day identifiers\n",
    "        ecg_dir: Path to ECG data directory\n",
    "        chunk_size: Size of each chunk\n",
    "        step_size: Step size for chunking\n",
    "        verbose: Print progress\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with features for all chunks\n",
    "    \"\"\"\n",
    "    all_features = []\n",
    "    \n",
    "    for person_day in person_days:\n",
    "        try:\n",
    "            if verbose:\n",
    "                print(f\"Processing day {person_day}...\")\n",
    "            \n",
    "            # Chunk the day\n",
    "            pd_result, chunks = tts.chunkify_day(\n",
    "                person_day=person_day,\n",
    "                chunk_size=chunk_size,\n",
    "                step_size=step_size,\n",
    "                ecg_dir=ecg_dir\n",
    "            )\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"  Chunked into {len(chunks)} chunks\")\n",
    "            \n",
    "            # Extract features\n",
    "            features = fe.extract_combined_features_sequential(\n",
    "                chunks,\n",
    "                ecg_column='EcgWaveform',\n",
    "                sampling_rate=250,\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            # Add metadata\n",
    "            features['person_day'] = person_day\n",
    "            features['person_id'] = person_day // 10\n",
    "            features['day'] = person_day % 10\n",
    "            \n",
    "            all_features.append(features)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"  Extracted {features.shape[1]-3} features\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"  ⚠ Error processing day {person_day}: {e}\")\n",
    "    \n",
    "    # Combine all features\n",
    "    if all_features:\n",
    "        combined = pd.concat(all_features, ignore_index=True)\n",
    "        # Handle NaN values\n",
    "        combined = combined.fillna(0)\n",
    "        return combined\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "print(\"✓ Pipeline function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline on demo days\n",
    "\n",
    "if ecg_path:\n",
    "    print(\"Running complete pipeline on demo days...\\n\")\n",
    "    try:\n",
    "        training_features = prepare_training_data(\n",
    "            person_days=params.DEMO_DAYS,\n",
    "            ecg_dir=ecg_path,\n",
    "            chunk_size=pd.Timedelta(minutes=5),\n",
    "            step_size=pd.Timedelta(minutes=1),\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n✓ Pipeline complete\")\n",
    "        print(f\"  Final feature matrix shape: {training_features.shape}\")\n",
    "        print(f\"  Total chunks: {len(training_features)}\")\n",
    "        print(f\"  Feature columns: {training_features.shape[1]}\")\n",
    "        \n",
    "        display(training_features.head())\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Pipeline error: {e}\")\n",
    "        print(\"This is expected if data files don't exist locally.\")\n",
    "else:\n",
    "    print(\"⚠ ECG path not configured. Skipping pipeline.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. PyTorch Dataset Creation\n",
    "\n",
    "Create PyTorch datasets for deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PyTorch Dataset class\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class HypoglycemiaDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for hypoglycemia prediction.\n",
    "    \n",
    "    Args:\n",
    "        features: DataFrame or array of features\n",
    "        labels: Array of labels\n",
    "        transform: Optional transform to apply\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, features, labels, transform=None):\n",
    "        # Convert to numpy if DataFrame\n",
    "        if isinstance(features, pd.DataFrame):\n",
    "            self.features = features.values\n",
    "        else:\n",
    "            self.features = features\n",
    "            \n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get features and labels\n",
    "        features = self.features[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Convert to tensors\n",
    "        features_tensor = torch.tensor(features, dtype=torch.float32)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "        \n",
    "        # Apply transform if provided\n",
    "        if self.transform:\n",
    "            features_tensor = self.transform(features_tensor)\n",
    "        \n",
    "        return features_tensor, label_tensor\n",
    "\n",
    "print(\"✓ PyTorch Dataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample dataset\n",
    "\n",
    "if 'training_features' in locals() and not training_features.empty:\n",
    "    # Create dummy labels for demonstration\n",
    "    # In real use, these would come from glucose data\n",
    "    dummy_labels = np.random.randint(0, 2, size=len(training_features))\n",
    "    \n",
    "    # Select only numeric features (exclude metadata)\n",
    "    feature_columns = [col for col in training_features.columns \n",
    "                      if col not in ['person_day', 'person_id', 'day']]\n",
    "    features_numeric = training_features[feature_columns]\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = HypoglycemiaDataset(features_numeric, dummy_labels)\n",
    "    \n",
    "    print(f\"✓ Dataset created\")\n",
    "    print(f\"  Number of samples: {len(dataset)}\")\n",
    "    print(f\"  Feature dimension: {features_numeric.shape[1]}\")\n",
    "    \n",
    "    # Create DataLoader\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    print(f\"  Batch size: 32\")\n",
    "    print(f\"  Number of batches: {len(dataloader)}\")\n",
    "    \n",
    "    # Test the dataloader\n",
    "    for batch_features, batch_labels in dataloader:\n",
    "        print(f\"\\nFirst batch:\")\n",
    "        print(f\"  Features shape: {batch_features.shape}\")\n",
    "        print(f\"  Labels shape: {batch_labels.shape}\")\n",
    "        break\n",
    "else:\n",
    "    print(\"⚠ Training features not available. Skipping dataset creation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Error Handling and Debugging Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common debugging checks\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DEBUGGING CHECKLIST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check environment variables\n",
    "print(\"\\n1. Environment Variables:\")\n",
    "print(f\"   GLUCOSE_PATH: {os.getenv('GLUCOSE_PATH')}\")\n",
    "print(f\"   ECG_PATH: {os.getenv('ECG_PATH')}\")\n",
    "\n",
    "# Check paths exist\n",
    "print(\"\\n2. Path Validation:\")\n",
    "if os.getenv('GLUCOSE_PATH'):\n",
    "    print(f\"   Glucose path exists: {Path(os.getenv('GLUCOSE_PATH')).exists()}\")\n",
    "if os.getenv('ECG_PATH'):\n",
    "    print(f\"   ECG path exists: {Path(os.getenv('ECG_PATH')).exists()}\")\n",
    "\n",
    "# Check module imports\n",
    "print(\"\\n3. Module Imports:\")\n",
    "print(\"   ✓ hypopredict.compressor\")\n",
    "print(\"   ✓ hypopredict.train_test_split\")\n",
    "print(\"   ✓ hypopredict.feature_extraction\")\n",
    "print(\"   ✓ hypopredict.person\")\n",
    "print(\"   ✓ hypopredict.cv\")\n",
    "print(\"   ✓ hypopredict.params\")\n",
    "\n",
    "# Check data availability\n",
    "print(\"\\n4. Data Status:\")\n",
    "print(f\"   Person object: {'✓ Created' if 'person' in locals() and person else '✗ Not created'}\")\n",
    "print(f\"   Chunks available: {'✓ Yes' if 'chunks' in locals() and chunks else '✗ No'}\")\n",
    "print(f\"   Features extracted: {'✓ Yes' if 'training_features' in locals() else '✗ No'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Issues and Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display troubleshooting guide\n",
    "\n",
    "troubleshooting = \"\"\"\n",
    "COMMON ISSUES AND SOLUTIONS:\n",
    "\n",
    "1. ImportError: No module named 'hypopredict'\n",
    "   Solution: Install package in editable mode: pip install -e .\n",
    "\n",
    "2. Environment variables not loading\n",
    "   Solution: Ensure .env file exists and run: load_dotenv()\n",
    "\n",
    "3. FileNotFoundError when loading data\n",
    "   Solution: \n",
    "   - Check .env paths are correct and absolute\n",
    "   - Verify data files exist in specified directories\n",
    "   - Try loading from Google Drive instead: glucose_src='gdrive'\n",
    "\n",
    "4. NeuroKit2 RuntimeWarnings during feature extraction\n",
    "   Solution: This is normal for chunks with insufficient R-peaks\n",
    "   - Suppress warnings: warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "   - Handle NaN values in features: features.fillna(0)\n",
    "\n",
    "5. Memory issues with large datasets\n",
    "   Solution:\n",
    "   - Process days one at a time\n",
    "   - Save features to disk incrementally\n",
    "   - Use smaller chunk sizes or fewer days\n",
    "\n",
    "6. Jupyter kernel crashes\n",
    "   Solution:\n",
    "   - Restart kernel: Kernel → Restart Kernel\n",
    "   - Install ipykernel: pip install ipykernel\n",
    "   - Select correct kernel: Python (hypopredict)\n",
    "\n",
    "7. Package changes not reflected\n",
    "   Solution:\n",
    "   - Reinstall package: pip uninstall hypopredict && pip install -e .\n",
    "   - Or use auto-reload in Jupyter:\n",
    "     %load_ext autoreload\n",
    "     %autoreload 2\n",
    "\"\"\"\n",
    "\n",
    "print(troubleshooting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. GCS/Cloud Training Considerations\n",
    "\n",
    "Tips for training on cloud platforms (Google Cloud, AWS, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud training configuration example\n",
    "\n",
    "cloud_training_guide = \"\"\"\n",
    "CLOUD TRAINING GUIDE:\n",
    "\n",
    "1. Upload Data to GCS Bucket:\n",
    "   gsutil cp -r /local/glucose/data gs://your-bucket/hypopredict-data/glucose/\n",
    "   gsutil cp -r /local/ecg/data gs://your-bucket/hypopredict-data/ecg/\n",
    "\n",
    "2. Set Environment Variables in Training Script:\n",
    "   import os\n",
    "   os.environ['GLUCOSE_PATH'] = '/gcs/your-bucket/hypopredict-data/glucose/'\n",
    "   os.environ['ECG_PATH'] = '/gcs/your-bucket/hypopredict-data/ecg/'\n",
    "\n",
    "3. Use the Same Code:\n",
    "   # Everything works the same way!\n",
    "   from hypopredict.person import Person\n",
    "   person = Person(ID=1, ecg_dir=os.getenv('ECG_PATH'))\n",
    "   person.load_HG_data(glucose_src='local')\n",
    "\n",
    "4. Alternative: Use Google Drive Links (for smaller datasets):\n",
    "   import hypopredict.compressor as comp\n",
    "   glucose_df = comp.gdrive_to_pandas(comp.GLUCOSE_ID_LINKS[0])\n",
    "\n",
    "5. Save Models to GCS:\n",
    "   import joblib\n",
    "   joblib.dump(model, '/gcs/your-bucket/models/model.pkl')\n",
    "\n",
    "6. Track Experiments with MLflow:\n",
    "   import mlflow\n",
    "   mlflow.set_tracking_uri('gs://your-bucket/mlflow')\n",
    "   mlflow.log_params({...})\n",
    "   mlflow.log_metrics({...})\n",
    "\"\"\"\n",
    "\n",
    "print(cloud_training_guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. ✓ Package setup and imports\n",
    "2. ✓ Data loading from multiple sources (Google Drive, local, GCS)\n",
    "3. ✓ Person class usage for patient data management\n",
    "4. ✓ Data chunking for time-series processing\n",
    "5. ✓ Feature extraction (statistical, ECG, HRV)\n",
    "6. ✓ Cross-validation setup\n",
    "7. ✓ Complete training pipeline\n",
    "8. ✓ PyTorch dataset creation\n",
    "9. ✓ Error handling and debugging\n",
    "10. ✓ Cloud training considerations\n",
    "\n",
    "**Next Steps:**\n",
    "- Review `MIGRATION.md` for detailed migration guide\n",
    "- Check `QUICK_REFERENCE.md` for quick code snippets\n",
    "- Read `TESTING_LOCALLY.md` for local setup instructions\n",
    "- Start building your models with the prepared features!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
